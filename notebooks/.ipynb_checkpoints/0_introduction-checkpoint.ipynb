{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Amazon SageMaker Solution: Explaining Credit Decisions\n", "\n", "Given the increasing complexity of machine learning models, the need for\n", "model explainability has been growing lately. Some governments have also\n", "introduced stricter regulations that mandate a *right to explanation*\n", "from machine learning models. In this solution, we take a look at how\n", "[Amazon SageMaker](https://aws.amazon.com/sagemaker/) can be used to\n", "explain individual predictions from machine learning models, and also\n", "shine a light on the global behavior of models too."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Credit Default Classification\n", "As an example use-case, we classify credit applications and predict\n", "whether the credit would be payed back or not (often called a *credit\n", "default*). Given a credit application from a bank customer, the aim of\n", "the bank is to predict whether or not the customer will pay back the\n", "credit in accordance with their repayment plan. When a customer can't pay\n", "back their credit, often called a 'default', the bank loses money and the\n", "customers credit score will be impacted. On the other hand, denying\n", "trustworthy customers credit also has a set of negative impacts. Using\n", "accurate machine learning models to classify the risk of a credit\n", "application can help find a good balance between these two scenarios, but\n", "this provides no comfort to those customers who have been denied credit.\n", "\n", "Using explanability methods, it's possible to determine actionable\n", "factors that had a negative impact on the application. Customers can then\n", "take action to increase their chance of obtaining credit in subsequent\n", "applications. Companies can also use explanations to identify risk\n", "factors.\n", "\n", "We train a tree-based\n", "[LightGBM](https://lightgbm.readthedocs.io/en/latest/) model using\n", "[Amazon SageMaker](https://aws.amazon.com/sagemaker/) and explain its\n", "predictions using a game theoretic approach called\n", "[SHAP](https://github.com/slundberg/shap) (SHapley Additive\n", "exPlanations). We deploy a endpoint that returns the credit default risk\n", "score, alongside an explanation, in real-time. We also show how\n", "explanations can be computed in batch mode."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## What is SHAP?\n", "SHAP is the method used for calculating explanations in this solution.\n", "Unlike other feature attribution methods, such as single feature\n", "permutation, SHAP tries to disentangle the effect of a single feature by\n", "looking at all possible combinations of features.\n", "\n", "[SHAP](https://github.com/slundberg/shap) (Lundberg et al. 2017) stands\n", "for SHapley Additive exPlanations. 'Shapley' relates to a game theoretic\n", "concept called [Shapley\n", "values](https://en.wikipedia.org/wiki/Shapley_value) that is used to\n", "create the explanations. A Shapley value describes the marginal\n", "contribution of each 'player' when considering all possible 'coalitions'.\n", "Using this in a machine learning context, a Shapley value  describes the\n", "marginal contribution of each feature when considering all possible sets\n", "of features. 'Additive' relates to the fact that these Shapley values can\n", "be summed together to give the final model prediction.\n", "\n", "As an example, we might start off with a baseline credit default risk of\n", "10%. Given a set of features, we can calculate the Shapley value for each\n", "feature. Summing together all the Shapley values, we might obtain a\n", "cumulative value of +30%. Given the same set of features, we therefore\n", "expect our model to return a credit default risk of 40% (i.e. 10% + 30%)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Architecture\n", "\n", "As part of the solution, the following services are used:\n", "\n", "* [AWS Lambda](https://aws.amazon.com/lambda/): Used to generate a synthetic credits dataset and upload to Amazon S3.\n", "* [AWS Glue](https://aws.amazon.com/glue/): Used to crawl datasets, and transform the credits dataset using Apache Spark.\n", "* [Amazon S3](https://aws.amazon.com/s3/): Used to store datasets and the outputs of the AWS Glue Job.\n", "* [Amazon SageMaker Notebook](https://aws.amazon.com/sagemaker/): Used to train the LightGBM model.\n", "* [Amazon ECR](https://aws.amazon.com/ecr/): Used to store the custom Scikit-learn + LightGBM training environment.\n", "* [Amazon SageMaker Endpoint](https://aws.amazon.com/sagemaker/): Used to deploy the trained model and SHAP explainer.\n", "* [Amazon SageMaker Batch Transform](https://aws.amazon.com/sagemaker/): Used to compute explanations in batch.\n", "\n", "<p align=\"center\">\n", "  <img src=\"https://github.com/awslabs/sagemaker-explaining-credit-decisions/raw/master/docs/architecture_diagrams/complete.png\" width=\"1000px\">\n", "</p>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Stages\n", "\n", "Our solution is split into the following stages, and each stage has it's own notebook:\n", "\n", "* [Introduction](./0_introduction.ipynb): We take a high-level look at the solution components.\n", "* [Datasets](./1_datasets.ipynb): We prepare a dataset for machine learning using AWS Glue.\n", "* [Training](./2_training.ipynb): We train a LightGBM model using Amazon SageMaker, so we have an example trained model to explain.\n", "* [Endpoint](./3_endpoint.ipynb): We deploy the model explainer to a HTTP endpoint using Amazon SageMaker and visualize the explanations.\n", "* [Batch Transform](./4_batch_transform.ipynb): We use Amazon SageMaker Batch Transform to obtain explanations for our complete dataset.\n", "* [Dashboard](./5_dashboard.ipynb): We develop a dashboard for explanations using Amazon SageMaker and Streamlit.\n", "* [Conclusion](./6_conclusion.ipynb): We wrap things up and discuss how to clean up the solution."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Next Stage\n", "\n", "Up next we'll take a look at preparing datasets for machine learning using AWS Glue.\n", "\n", "[Click here to continue.](./1_datasets.ipynb)"]}], "metadata": {"jupytext": {"cell_metadata_filter": "-all", "main_language": "python", "notebook_metadata_filter": "-all"}, "kernelspec": {"display_name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:793310587911:image/sagemaker-jumpstart-data-science-1.0", "language": "python", "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:793310587911:image/sagemaker-jumpstart-data-science-1.0"}}, "nbformat": 4, "nbformat_minor": 4}