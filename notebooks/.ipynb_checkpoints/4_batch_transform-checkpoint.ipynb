{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Batch Transform for Explanations\n", "\n", "In this notebook, we'll use Amazon SageMaker Batch Transform to obtain\n", "explanations for our complete dataset.\n", "\n", "**Note**: When running this notebook on SageMaker Studio, you should make\n", "sure the 'SageMaker JumpStart Data Science 1.0' image/kernel is used. You\n", "can run all cells or step through them one at a time.\n", "\n", "<p align=\"center\">\n", "  <img src=\"https://github.com/awslabs/sagemaker-explaining-credit-decisions/raw/master/docs/architecture_diagrams/stage_4.png\" width=\"1000px\">\n", "</p>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We then import a variety of packages that will be used throughout\n", "the notebook. One of the most important packages used throughout this\n", "solution is the Amazon SageMaker Python SDK (i.e. `import sagemaker`). We\n", "also import modules from our own custom package that can be found at\n", "`./package`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import boto3\n", "from pathlib import Path\n", "import sagemaker\n", "from sagemaker.transformer import Transformer\n", "import sys\n", "\n", "sys.path.insert(0, '../package')\n", "from package import config, utils"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Up next, we define the current folder, a sagemaker session and a\n", "sagemaker client (from `boto3`)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["current_folder = utils.get_current_folder(globals())\n", "sagemaker_session = sagemaker.Session()\n", "sagemaker_client = boto3.client('sagemaker')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We define a function below to retrieve the same model that was created in\n", "last stage. Model refers to the package of model assets and deployment\n", "code. We could have created another model here (using the same model data\n", "from the training stage) but let's use the same model to avoid\n", "duplication."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_latest_model(name_contains):\n", "    paginator = sagemaker_client.get_paginator('list_models')\n", "    try:\n", "        for page in paginator.paginate(NameContains=name_contains):\n", "            models = page['Models']\n", "            if len(models):\n", "                return models[0]['ModelName']\n", "    except:\n", "        raise ValueError(\"Couldn't find any models with '{}' in name.\".format(name_contains))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["latest_model = get_latest_model(config.SOLUTION_PREFIX)\n", "print(\"latest model: {}\".format(latest_model))\n", "job_name = latest_model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Unlike the last stage, where we deployed an endpoint, we define a\n", "`Transformer` to perform the batch computation. We specify the instance\n", "type that should be used for the computation (i.e. `ml.c5.xlarge`) and a\n", "number of other parameters. `strategy='SingleRecord'` means that records\n", "will be processed by the explainer one at a time. And `output_path`\n", "defines where the Batch Transform output should be saved."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["batch_explainer = Transformer(\n", "    model_name=latest_model,\n", "    instance_count=1,\n", "    instance_type='ml.c5.xlarge',\n", "    strategy='SingleRecord',\n", "    assemble_with='Line',\n", "    output_path='s3://' + str(Path(config.S3_BUCKET, 'explanations', job_name)) + '/',\n", "    accept='application/json',\n", "    base_transform_job_name=config.SOLUTION_PREFIX,\n", "    sagemaker_session=sagemaker_session,\n", "    tags=[{'Key': config.TAG_KEY, 'Value': config.SOLUTION_PREFIX}]\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We haven't yet started the Batch Transform Job. Calling `.transform` does\n", "that below. We also specify the `content_type` at this stage, which gives\n", "us control over what type of entities we want to return from the\n", "explainer. As an example, we have requested SHAP interaction values\n", "during this batch job."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["entities = [\n", "    'data',\n", "    'features',\n", "    'prediction',\n", "    'explanation_shap_values',\n", "    'explanation_shap_interaction_values'\n", "]\n", "batch_explainer.transform(\n", "    data='s3://' + str(Path(config.S3_BUCKET, config.DATASETS_S3_PREFIX, 'data_test')) + '/',\n", "    content_type=\"application/json; entities={}\".format(\",\".join(entities)),\n", "    split_type='Line',\n", "    wait=True\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["After the Batch Transform Job has completed successfully, we will have a\n", "complete set of explanations sitting in the Amazon S3 bucket."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Next Stage\n", "\n", "Up next we'll wrap things up and discuss how to clean up the solution.\n", "\n", "[Click here to continue.](./5_conclusion.ipynb)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"jupytext": {"cell_metadata_filter": "-all", "main_language": "python", "notebook_metadata_filter": "-all"}, "kernelspec": {"display_name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:793310587911:image/sagemaker-jumpstart-data-science-1.0", "language": "python", "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:793310587911:image/sagemaker-jumpstart-data-science-1.0"}}, "nbformat": 4, "nbformat_minor": 4}